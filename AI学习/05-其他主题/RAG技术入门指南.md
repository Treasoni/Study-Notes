---
tags: [ai, RAG, 检索增强生成, 知识库]
---

# RAG 技术入门指南

> [!info] 说明
> 本指南面向已有 AI 基础知识，想深入理解 RAG（检索增强生成）技术的开发者。重点在于**概念理解**，从基础到进阶技术循序渐进。

---

## 1. 基础概念层

### 1.1 什么是 RAG？

**RAG** 是 **Retrieval-Augmented Generation**（检索增强生成）的缩写。

简单来说，RAG 是一种技术架构，让大语言模型（LLM）能够**在生成回答时检索并引用外部知识库中的信息**。

#### 核心价值

RAG 让 AI 能够访问"外部知识"，突破训练数据的限制：

```
传统 LLM：只能依赖训练时学到的知识（闭卷考试）
    ↓
RAG 增强：可以查阅外部知识库再回答（开卷考试）
```

#### 通俗比喻

| 场景 | 传统 LLM | RAG 增强的 LLM |
|------|----------|----------------|
| 考试 | 闭卷考试，只能靠记忆 | 开卷考试，可以翻书查阅 |
| 回答 | 凭记忆回答，可能记错 | 先查资料再回答，更准确 |
| 知识 | 固定在训练数据中 | 可以实时更新知识库 |

---

### 1.2 为什么需要 RAG？

LLM 存在三大局限，RAG 正是为了解决这些问题：

#### 局限一：知识截止日期

```
问题：LLM 的训练数据有时间截止点
例如：GPT-4 的训练数据截止到 2023 年
结果：无法回答"昨天发生了什么新闻？"
```

#### 局限二：幻觉问题（Hallucination）

```
问题：LLM 可能"一本正经胡说八道"
表现：编造不存在的事实、引用不存在的论文
原因：模型在不确定时倾向于生成"看似合理"的内容
```

#### 局限三：无法访问私有数据

```
问题：企业内部文档、个人笔记等私有数据
原因：这些数据从未进入模型的训练集
结果：无法基于企业知识回答问题
```

#### RAG 如何解决？

| 局限 | RAG 解决方案 |
|------|-------------|
| 知识截止 | 实时检索最新知识库内容 |
| 幻觉问题 | 基于检索到的真实文档生成回答 |
| 私有数据 | 将私有数据存入知识库供检索 |

---

### 1.3 RAG 核心架构

RAG 的工作流程可以分为两个阶段：

#### 阶段一：知识库构建（离线）

```
原始文档 → 文本分块(Chunking) → 向量化(Embedding) → 存入向量数据库
```

#### 阶段二：检索与生成（在线）

```
用户问题
    ↓
向量化(Embedding) ──────────────────┐
    ↓                               │
向量检索（在向量数据库中找相似内容）    │ 相似度计算
    ↓                               │
检索到相关文档片段                    │
    ↓                               │
上下文组装（问题 + 检索结果）←────────┘
    ↓
LLM 生成回答
    ↓
最终回答（带引用来源）
```

#### 核心流程图

```
┌─────────────────────────────────────────────────────────────┐
│                      RAG 核心流程                            │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  【离线阶段】知识库构建                                       │
│                                                             │
│  文档 ──► 分块 ──► Embedding ──► 向量数据库                   │
│                                                             │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  【在线阶段】检索生成                                         │
│                                                             │
│  问题 ──► Embedding ──► 向量检索 ──► 上下文组装              │
│                                               │             │
│                                               ▼             │
│  回答 ◄── LLM 生成 ◄── 提示词构建                            │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 2. 技术组件层

### 2.1 Embedding（向量化）

#### 什么是向量嵌入？

Embedding 是将文本转换为**高维向量**的过程。这些向量能够捕捉文本的**语义信息**：

```
"猫是一种宠物"  →  [0.23, -0.45, 0.67, ...]  (1536维向量)
"狗是一种宠物"  →  [0.25, -0.43, 0.65, ...]  (语义相似，向量接近)
"汽车是交通工具" →  [0.12, 0.89, -0.34, ...]  (语义不同，向量远离)
```

#### 核心概念

- **语义相似性**：语义相近的文本，向量距离更近
- **向量空间**：所有文本被映射到同一个高维空间
- **相似度计算**：通常使用余弦相似度（Cosine Similarity）

#### 常用 Embedding 模型

| 模型 | 提供商 | 特点 | 维度 |
|------|--------|------|------|
| text-embedding-3-small | OpenAI | 性价比高，速度快 | 1536 |
| text-embedding-3-large | OpenAI | 效果最好 | 3072 |
| BGE (BAAI General Embedding) | BAAI | 开源，中文效果好 | 768/1024 |
| M3E | Moka | 开源，多语言支持 | 768 |
| Cohere Embed | Cohere | 支持多语言 | 1024 |

#### 中文 Embedding 选择建议

```
追求效果：BGE-large-zh 或 M3E-large
追求速度：BGE-small-zh 或 M3E-small
云服务：OpenAI text-embedding-3（需要网络）
本地部署：BGE / M3E（完全离线）
```

---

### 2.2 向量数据库

向量数据库专门用于存储和检索向量数据，支持**高效的相似性搜索**。

#### 主流向量数据库对比

| 数据库 | 类型 | 特点 | 适用场景 |
|--------|------|------|----------|
| **Pinecone** | 云服务 | 全托管，易用，扩展性好 | 生产环境、快速原型 |
| **Milvus** | 开源 | 高性能，可扩展，功能丰富 | 大规模生产环境 |
| **Qdrant** | 开源 | Rust 编写，高性能，轻量 | 中小规模、本地部署 |
| **Weaviate** | 开源 | 内置向量化，GraphQL API | 复杂查询场景 |
| **Chroma** | 开源 | 轻量级，Python 原生 | 快速原型、开发测试 |
| **FAISS** | 开源库 | Meta 出品，纯向量检索 | 嵌入应用、研究用途 |

#### 选择建议

```
快速开始/原型开发：
└─► Chroma（最简单）或 Qdrant（单机性能好）

生产环境（云服务）：
└─► Pinecone（免运维）或 Milvus（自托管）

大规模企业部署：
└─► Milvus（可扩展性强）

本地开发/嵌入式：
└─► FAISS 或 Chroma
```

---

### 2.3 Chunking（分块策略）

分块是将长文档切分成较小片段的过程，是 RAG 系统的**关键环节**。

#### 为什么需要分块？

```
问题1：文档太长，超过 LLM 上下文窗口
问题2：整篇文档作为检索单元，粒度太粗
解决：将文档切分成合适大小的块
```

#### 常见分块策略

##### 1. 固定大小分块（Fixed Size Chunking）

```
最简单的策略：按字符数或 token 数切分

示例（每 500 字符一块）：
"这是一段很长的文本..." → [块1: 500字符] [块2: 500字符] ...

优点：简单易实现
缺点：可能在句子中间切断，破坏语义
```

##### 2. 递归字符分块（Recursive Character Chunking）

```
按分隔符层级递归切分：
1. 先按段落（\n\n）切分
2. 如果块还是太大，按句子（\n）切分
3. 如果还是太大，按词切分

优点：尽量保持语义完整性
推荐：LangChain 的 RecursiveCharacterTextSplitter
```

##### 3. 语义分块（Semantic Chunking）

```
基于语义边界切分：
- 使用 Embedding 计算相邻句子的相似度
- 在语义变化大的地方切分

优点：语义边界更自然
缺点：计算成本高，需要额外 Embedding 调用
```

##### 4. 文档结构分块（Document Structure Chunking）

```
基于文档结构切分：
- Markdown：按标题层级（#、##、###）
- 代码：按函数、类
- HTML：按章节、段落

优点：保留文档逻辑结构
适用：结构化文档
```

#### 最佳实践

```
推荐配置：
- 块大小：300-500 tokens（约 500-800 中文字符）
- 重叠（Overlap）：100 tokens（约 150-200 字符）
- 策略：递归字符分块 + 文档结构感知

为什么需要 Overlap？
- 避免关键信息被切在两块之间
- 保持上下文连贯性

示例：
块1: [.....A.....][overlap]
块2:          [overlap][.....B.....]
                    ↑ 重复部分，确保信息连续
```

---

## 3. 进阶技术层

### 3.1 混合检索（Hybrid Search）

单一向量检索有其局限性，混合检索结合多种检索方式提升效果。

#### 向量检索 vs 关键词检索

| 方式 | 原理 | 优势 | 劣势 |
|------|------|------|------|
| 向量检索 | 语义相似度 | 理解同义词、语义 | 可能漏掉精确匹配 |
| 关键词检索 | BM25 等算法 | 精确匹配、专有名词 | 无法理解语义 |

#### 混合检索架构

```
用户问题
    │
    ├──► 向量检索 ──► 候选集 A（语义相关）
    │
    └──► 关键词检索 ──► 候选集 B（精确匹配）
              │
              ▼
         结果融合（RRF 或加权）
              │
              ▼
         最终候选集
              │
              ▼
         Reranking（重排序）
              │
              ▼
         Top-K 结果
```

#### Reranking（重排序）

```
为什么需要重排序？
- 初步检索可能返回大量候选
- 使用更精确的模型对候选重新排序
- 提升最终结果的相关性

常用 Reranker：
- Cohere Rerank（云服务）
- BGE Reranker（开源）
- Cross-Encoder 模型
```

---

### 3.2 查询增强技术

用户的原始问题可能表述不清或与知识库中的文档用词不一致，查询增强技术可以改善检索效果。

#### Multi-Query（多查询）

```
原始问题："如何提高性能？"

生成多个相关问题：
1. "性能优化的最佳实践是什么？"
2. "如何提升系统响应速度？"
3. "有哪些常见的性能瓶颈？"

对每个问题分别检索，合并结果
```

#### HyDE（假设文档嵌入）

```
思路：让 LLM 先生成一个"假设性答案"

用户问题："什么是 RAG？"
    ↓
LLM 生成假设答案：
"RAG（Retrieval-Augmented Generation）是一种将检索
与生成结合的技术..."
    ↓
对假设答案进行 Embedding
    ↓
用假设答案的向量去检索（更容易匹配到相关文档）
```

#### Step-Back Prompting

```
思路：先问一个更抽象的"后退问题"

原始问题："Python 3.11 有什么新特性？"
    ↓
后退问题："Python 的版本更新通常包含哪些类型的改进？"
    ↓
同时检索原始问题和后退问题的答案
    ↓
合并上下文生成最终回答
```

---

### 3.3 高级架构

#### RAPTOR（递归摘要树）

```
传统 RAG：只检索文档片段
问题：可能缺乏全局视角

RAPTOR 方案：
1. 对文档进行聚类
2. 为每个聚类生成摘要
3. 构建树状结构：叶子节点是原文，上层是摘要
4. 检索时同时搜索叶子和摘要节点

优点：同时获得细节和全局视角
```

#### GraphRAG（知识图谱 + RAG）

```
传统 RAG：基于文本片段检索
GraphRAG：构建知识图谱，基于实体关系检索

流程：
1. 从文档中提取实体和关系
2. 构建知识图谱
3. 检索时遍历图谱，获取相关实体和关系
4. 结合图谱信息和文本片段生成回答

优点：
- 更好的推理能力
- 可以回答"关系型"问题
- 减少幻觉

来源：Microsoft Research
```

#### Agentic RAG（智能体 RAG）

```
传统 RAG：检索 → 生成（单次流程）
Agentic RAG：让 AI Agent 自主决定检索策略

特点：
1. 自主反思：判断检索结果是否足够
2. 工具调用：可以调用搜索、数据库等多种工具
3. 迭代检索：根据中间结果调整检索策略
4. 多源整合：整合多个知识源

流程示例：
问题 → 初步检索 → 评估结果 →
(不够？) → 改写查询 → 再次检索 →
生成回答 → 自我验证 → (不满意？) → 迭代优化
```

---

## 4. 综合对比分析层

### 4.1 AI 能力增强方案全景图

RAG 只是增强 AI 能力的方案之一，让我们看看完整的方案图谱：

```
                    ┌─────────────────────────────────────┐
                    │         AI 能力增强方案              │
                    └─────────────────────────────────────┘
                                     │
        ┌────────────────┬───────────┼───────────┬────────────────┐
        ▼                ▼           ▼           ▼                ▼
   ┌─────────┐    ┌───────────┐ ┌─────────┐ ┌──────────┐  ┌───────────┐
   │   RAG   │    │Fine-tuning│ │  Long   │ │  Prompt  │  │   AI      │
   │检索增强 │    │  微调     │ │ Context │ │Engineering│  │  Skills   │
   └─────────┘    └───────────┘ └─────────┘ └──────────┘  └───────────┘
```

---

### 4.2 五种方案详细对比

| 维度 | RAG | Fine-tuning | Long Context | Prompt Engineering | AI Skills |
|------|-----|-------------|--------------|-------------------|-----------|
| **核心原理** | 检索外部知识注入上下文 | 调整模型参数 | 扩大上下文窗口 | 优化输入提示词 | 预定义任务模板 |
| **知识更新** | ⚡ 实时 | 🐢 需重新训练 | ⚡ 实时 | ⚡ 实时 | ⚡ 实时 |
| **成本** | 💰 低 | 💰💰💰 高 | 💰💰 中 | 💰 低 | 💰 低 |
| **数据隐私** | ✅ 可本地部署 | ⚠️ 需上传数据 | ✅ 可控 | ✅ 可控 | ✅ 可控 |
| **技术门槛** | 中 | 高 | 低 | 低 | 低 |
| **最佳场景** | 企业知识库、FAQ | 领域专用、风格定制 | 合同审查、长文档 | 快速原型、简单任务 | 重复性工作流 |

---

### 4.3 各方案深入分析

#### RAG（检索增强生成）

**优势**：
- 知识可实时更新，无需重新训练
- 可以引用来源，增强可信度
- 适合大规模知识库
- 成本相对较低

**劣势**：
- 需要构建和维护检索系统
- 检索质量直接影响最终效果
- 复杂查询可能检索不准

**适用场景**：
- 企业内部知识库
- 客服问答系统
- 法律/医疗等专业问答
- 技术文档查询

---

#### Fine-tuning（微调）

**优势**：
- 领域深度定制，效果显著
- 输出风格可控
- 推理时无需额外检索开销

**劣势**：
- 训练成本高（GPU、数据标注）
- 知识固定在训练时刻
- 需要大量高质量标注数据
- 更新知识需重新训练

**适用场景**：
- 医疗诊断辅助
- 法律文书生成
- 品牌风格写作
- 特定领域专家系统

---

#### Long Context（长上下文）

**优势**：
- 实现简单，无需分块处理
- 保留完整上下文，不丢失信息
- 适合处理单个长文档

**劣势**：
- API 成本随上下文长度增加
- 存在"中间迷失"问题（Lost in the Middle）
- 不适合大规模知识库
- 可扩展性差

**适用场景**：
- 合同审查与分析
- 整本书籍分析
- 代码库理解
- 长报告总结

> [!note] 中间迷失问题
> 研究表明，LLM 对位于上下文中间的信息往往关注度较低，导致"中间迷失"现象。

---

#### Prompt Engineering（提示词工程）

**优势**：
- 零成本，即时生效
- 灵活性高，可快速迭代
- 不依赖额外基础设施

**劣势**：
- 效果有上限
- 高度依赖用户技巧
- 结果不稳定，难以复现
- 无法解决知识缺失问题

**适用场景**：
- 快速原型验证
- 简单任务处理
- 探索性对话
- 学习和实验

---

#### AI Skills（智能体技能）

**优势**：
- 可复用、可分享
- 标准化工作流程
- 降低使用门槛
- 可组合、可扩展

**劣势**：
- 需要预先定义
- 灵活性有限
- 适用场景需要识别

**适用场景**：
- 代码审查
- 文档生成
- 重复性工作流
- 特定任务自动化

---

### 4.4 组合策略（2025年最佳实践）

不同场景下，多种方案可以组合使用：

```
┌─────────────────────────────────────────────────────────────┐
│                    推荐组合策略                              │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  场景1：企业知识助手                                         │
│  └─► RAG + Prompt Engineering + AI Skills                  │
│      （实时知识 + 优化提示 + 标准化流程）                     │
│                                                             │
│  场景2：领域专家系统                                         │
│  └─► Fine-tuning + RAG                                     │
│      （领域深度 + 实时知识更新）                              │
│                                                             │
│  场景3：代码开发助手                                         │
│  └─► Long Context + AI Skills + Prompt Engineering         │
│      （完整代码上下文 + 标准化任务 + 灵活提示）                │
│                                                             │
│  场景4：客服机器人                                           │
│  └─► RAG + Fine-tuning + AI Skills                         │
│      （知识检索 + 风格定制 + 标准回复）                       │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

### 4.5 选择决策树

```
你的需求是什么？
│
├─ 知识需要频繁更新？
│   └─► 是 → RAG（首选）
│
├─ 需要特定输出风格/格式？
│   └─► 是 → Fine-tuning（可叠加 RAG 提供实时知识）
│
├─ 处理单个长文档（如合同）？
│   └─► 是 → Long Context
│
├─ 有重复性任务需要标准化？
│   └─► 是 → AI Skills
│
└─ 快速测试/简单任务？
    └─► 是 → Prompt Engineering
```

---

## 5. 实践指南层

### 5.1 RAG 成熟度路径

从简单到复杂，RAG 系统可以分为五个成熟度级别：

```
Level 1: Naive RAG（基础 RAG）
├─ 简单的文档分块
├─ 基础向量检索
└─ 直接生成回答
│
Level 2: Smart Chunking（智能分块）
├─ 语义感知分块
├─ 元数据增强（标题、来源、时间）
└─ 分块优化策略
│
Level 3: Hybrid Search（混合检索）
├─ 向量检索 + 关键词检索
├─ Reranking 重排序
└─ 查询增强（Multi-Query、HyDE）
│
Level 4: GraphRAG（结构化知识）
├─ 知识图谱构建
├─ 实体关系推理
└─ 结构化知识查询
│
Level 5: Agentic RAG（智能体 RAG）
├─ 自主反思与迭代
├─ 多工具调用
└─ 动态检索策略
```

#### 建议起步路径

```
第一步：实现 Level 1（验证可行性）
    ↓
第二步：优化到 Level 2（提升基础效果）
    ↓
第三步：根据需求选择 Level 3-5（针对性优化）
```

---

### 5.2 常见问题与解决方案

#### 问题1：检索不到相关内容

```
可能原因：
├─ 分块太大或太小
├─ Embedding 模型不适合领域
├─ 查询与文档用词差异大
└─ 知识库本身缺少相关内容

解决方案：
├─ 调整分块大小（尝试 300-500 tokens）
├─ 尝试领域专用 Embedding 模型
├─ 使用查询增强（Multi-Query、HyDE）
└─ 扩充知识库内容
```

#### 问题2：检索到无关内容

```
可能原因：
├─ 向量相似度阈值设置不当
├─ 分块语义不明确
└─ 缺乏重排序

解决方案：
├─ 提高相似度阈值
├─ 添加元数据过滤
├─ 引入 Reranking 重排序
└─ 优化分块策略
```

#### 问题3：回答不准确

```
可能原因：
├─ 检索结果质量差
├─ 提示词设计不当
├─ 上下文过长导致"迷失"
└─ LLM 能力不足

解决方案：
├─ 提升检索质量（见上述问题）
├─ 优化提示词模板
├─ 限制上下文长度，优先高质量片段
└─ 升级 LLM 模型
```

#### 问题4：响应速度慢

```
可能原因：
├─ 向量数据库查询慢
├─ Embedding 计算耗时
├─ LLM 推理慢
└─ 检索结果太多

解决方案：
├─ 优化向量数据库索引
├─ 缓存常见查询的 Embedding
├─ 减少 LLM 输入长度
└─ 限制检索返回数量（Top-K）
```

---

### 5.3 工具与框架推荐

#### RAG 框架

| 框架 | 特点 | 适用场景 |
|------|------|----------|
| **LlamaIndex** | 专注 RAG，文档丰富 | RAG 应用开发首选 |
| **LangChain** | 通用 LLM 框架，生态丰富 | 复杂 LLM 应用 |
| **Haystack** | 生产级 NLP 框架 | 企业级搜索系统 |
| **DSPy** | 声明式 LLM 编程 | 研究、优化提示 |

#### 快速开始示例

```python
# LlamaIndex 快速示例
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

# 1. 加载文档
documents = SimpleDirectoryReader("./docs").load_data()

# 2. 构建索引
index = VectorStoreIndex.from_documents(documents)

# 3. 创建查询引擎
query_engine = index.as_query_engine()

# 4. 查询
response = query_engine.query("什么是 RAG？")
print(response)
```

#### 学习资源

| 资源 | 说明 |
|------|------|
| [RAG Techniques GitHub](https://github.com/NirDiamant/RAG_Techniques) | RAG 技术大全，含代码示例 |
| [GraphRAG Microsoft](https://microsoft.github.io/graphrag/) | 微软 GraphRAG 官方文档 |
| [LlamaIndex Documentation](https://docs.llamaindex.ai/) | LlamaIndex 官方文档 |
| [LangChain RAG Tutorial](https://python.langchain.com/docs/tutorials/rag/) | LangChain RAG 教程 |

---

## 6. 总结

### 核心要点回顾

```
1. RAG 是什么
   └─► 检索增强生成，让 LLM 能访问外部知识

2. 为什么需要 RAG
   └─► 解决知识时效性、幻觉、私有数据三大问题

3. 核心组件
   ├─► Embedding：文本向量化
   ├─► 向量数据库：存储和检索向量
   └─► Chunking：文档分块策略

4. 进阶技术
   ├─► 混合检索：向量 + 关键词
   ├─► 查询增强：Multi-Query、HyDE
   └─► 高级架构：GraphRAG、Agentic RAG

5. 方案选择
   └─► 根据场景选择 RAG / Fine-tuning / Long Context 等
```

### 学习路径建议

```
入门 → 理解 RAG 基本概念和流程
  ↓
实践 → 用 LlamaIndex 搭建简单 RAG 系统
  ↓
优化 → 调整分块策略，尝试不同 Embedding
  ↓
进阶 → 实现混合检索、Reranking
  ↓
高级 → 探索 GraphRAG、Agentic RAG
```

---

## 相关文档

- [[01-基础概念/人工智能重要的六大概念体系]] - AI 核心概念基础
- [[03-进阶应用/Claude MCP 使用指南]] - MCP 配置相关

---

*最后更新：2026-02-27*
